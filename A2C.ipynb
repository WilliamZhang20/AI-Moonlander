{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjp4lzDF+NTg9E5m9JahTK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xlgu2WZiBShC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "20ec37d0-1f20-4a54-b10d-ce1673ace934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (696 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y build-essential swig libopenmpi-dev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "Zyr5Td55Bbw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d326e9ac-1700-41ad-f41a-34842334333d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351178 sha256=315472ecb5f883d305130a276a36983429bfb068dc42097b0d3c944b4642228d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        return action_probs, value\n",
        "\n",
        "def record_video(agent, filename=\"lunar_lander.mp4\", max_steps=1000):\n",
        "    env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "    state, _ = env.reset()\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        action_probs, value = agent(state_tensor)\n",
        "        action_dist = torch.distributions.Categorical(action_probs)\n",
        "        action = action_dist.sample()\n",
        "\n",
        "        state, _, terminated, truncated, _ = env.step(action.item())\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "    print(f\"Video saved as {filename}\")\n",
        "\n",
        "def a2c(env, num_episodes, lr=0.001, gamma=0.99):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = ActorCritic(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_rewards = []\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            action_probs, value = model(state_tensor)\n",
        "            action_dist = torch.distributions.Categorical(action_probs)\n",
        "            action = action_dist.sample()\n",
        "            log_prob = action_dist.log_prob(action)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_rewards.append(reward)\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "\n",
        "        R = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8) # Normalize returns\n",
        "\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "\n",
        "        for log_prob, value, ret in zip(log_probs, values, returns):\n",
        "            advantage = ret - value.squeeze()\n",
        "            actor_loss = -log_prob * advantage\n",
        "            critic_loss = (ret - value.squeeze()) ** 2\n",
        "\n",
        "            actor_losses.append(actor_loss)\n",
        "            critic_losses.append(critic_loss)\n",
        "\n",
        "        actor_loss = torch.stack(actor_losses).mean()\n",
        "        critic_loss = torch.stack(critic_losses).mean()\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_reward = sum(episode_rewards)\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"LunarLander-v3\")\n",
        "    model = a2c(env, num_episodes=1000)\n",
        "    env.close()\n",
        "    record_video(model)"
      ],
      "metadata": {
        "id": "p93yxK50BhWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045f515c-bf3a-4e24-be7c-3fe0230ee36b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -143.44459430416927\n",
            "Episode 11, Total Reward: -80.8122206671884\n",
            "Episode 21, Total Reward: -504.9605307767998\n",
            "Episode 31, Total Reward: -539.0449824176541\n",
            "Episode 41, Total Reward: -135.97904181446552\n",
            "Episode 51, Total Reward: -63.081606443897236\n",
            "Episode 61, Total Reward: -153.3198251042425\n",
            "Episode 71, Total Reward: -181.87435596561232\n",
            "Episode 81, Total Reward: -81.17411488928859\n",
            "Episode 91, Total Reward: -143.47633967183455\n",
            "Episode 101, Total Reward: -204.5243730045829\n",
            "Episode 111, Total Reward: -44.66312456864934\n",
            "Episode 121, Total Reward: -34.95299309396995\n",
            "Episode 131, Total Reward: -62.49685196126113\n",
            "Episode 141, Total Reward: -134.24189648438406\n",
            "Episode 151, Total Reward: -30.56108172166992\n",
            "Episode 161, Total Reward: -63.092337773548756\n",
            "Episode 171, Total Reward: -304.85197182301187\n",
            "Episode 181, Total Reward: 21.78769002773491\n",
            "Episode 191, Total Reward: 9.823293397086431\n",
            "Episode 201, Total Reward: -175.4219761726331\n",
            "Episode 211, Total Reward: -94.61616182813339\n",
            "Episode 221, Total Reward: -202.10650496032468\n",
            "Episode 231, Total Reward: -226.53027914211015\n",
            "Episode 241, Total Reward: -30.631931746502147\n",
            "Episode 251, Total Reward: -108.98040698603859\n",
            "Episode 261, Total Reward: 35.497950518106876\n",
            "Episode 271, Total Reward: -21.161618166660432\n",
            "Episode 281, Total Reward: 50.281464680648774\n",
            "Episode 291, Total Reward: -61.61205611945526\n",
            "Episode 301, Total Reward: -164.3214029785937\n",
            "Episode 311, Total Reward: 28.928151862903576\n",
            "Episode 321, Total Reward: -90.5482491112817\n",
            "Episode 331, Total Reward: -15.951111143046887\n",
            "Episode 341, Total Reward: 15.296452256060205\n",
            "Episode 351, Total Reward: 30.585492572252026\n",
            "Episode 361, Total Reward: 57.63959242064891\n",
            "Episode 371, Total Reward: -63.887365991410576\n",
            "Episode 381, Total Reward: 19.65193116203513\n",
            "Episode 391, Total Reward: 69.05160302888788\n",
            "Episode 401, Total Reward: 61.908481696552286\n",
            "Episode 411, Total Reward: 32.26068526438061\n",
            "Episode 421, Total Reward: 39.49147353685663\n",
            "Episode 431, Total Reward: 97.3173526266075\n",
            "Episode 441, Total Reward: 68.9263388489077\n",
            "Episode 451, Total Reward: -78.1160643827162\n",
            "Episode 461, Total Reward: 54.64696776489426\n",
            "Episode 471, Total Reward: 65.93885169640726\n",
            "Episode 481, Total Reward: 67.84046241739176\n",
            "Episode 491, Total Reward: 11.217615724168269\n",
            "Episode 501, Total Reward: 243.8611726956094\n",
            "Episode 511, Total Reward: 162.97153662686412\n",
            "Episode 521, Total Reward: 3.398564536455627\n",
            "Episode 531, Total Reward: 192.3844627186661\n",
            "Episode 541, Total Reward: 51.737961446558586\n",
            "Episode 551, Total Reward: -29.60717267502487\n",
            "Episode 561, Total Reward: 202.65211018794156\n",
            "Episode 571, Total Reward: -105.79523921510244\n",
            "Episode 581, Total Reward: -60.618866013211246\n",
            "Episode 591, Total Reward: -64.30164450369779\n",
            "Episode 601, Total Reward: 196.50627785406755\n",
            "Episode 611, Total Reward: -30.645745191448\n",
            "Episode 621, Total Reward: -24.05669629862723\n",
            "Episode 631, Total Reward: -78.7843570011852\n",
            "Episode 641, Total Reward: 125.6376552012484\n",
            "Episode 651, Total Reward: -129.22325698210526\n",
            "Episode 661, Total Reward: -25.655591585148986\n",
            "Episode 671, Total Reward: 150.5018673051641\n",
            "Episode 681, Total Reward: 181.11982752925138\n",
            "Episode 691, Total Reward: -137.78330501583832\n",
            "Episode 701, Total Reward: 141.42488841848478\n",
            "Episode 711, Total Reward: 37.51060087108914\n",
            "Episode 721, Total Reward: -172.29527168806504\n",
            "Episode 731, Total Reward: -150.50229018649742\n",
            "Episode 741, Total Reward: 255.73766105544493\n",
            "Episode 751, Total Reward: -12.253450316477398\n",
            "Episode 761, Total Reward: -45.9442926506932\n",
            "Episode 771, Total Reward: -48.0308886778249\n",
            "Episode 781, Total Reward: -78.97881476586014\n",
            "Episode 791, Total Reward: -87.85229136030004\n",
            "Episode 801, Total Reward: -23.796994586898087\n",
            "Episode 811, Total Reward: -11.473007342845847\n",
            "Episode 821, Total Reward: 99.26992399471905\n",
            "Episode 831, Total Reward: 23.783316807249776\n",
            "Episode 841, Total Reward: 71.96589688102712\n",
            "Episode 851, Total Reward: -53.34423889103249\n",
            "Episode 861, Total Reward: -152.73168773254082\n",
            "Episode 871, Total Reward: 204.0682738760555\n",
            "Episode 881, Total Reward: 195.687245969515\n",
            "Episode 891, Total Reward: -74.5255182409684\n",
            "Episode 901, Total Reward: 144.66120436997574\n",
            "Episode 911, Total Reward: 18.24663896558677\n",
            "Episode 921, Total Reward: 25.651729882162098\n",
            "Episode 931, Total Reward: 173.99937171224468\n",
            "Episode 941, Total Reward: 211.58233011776184\n",
            "Episode 951, Total Reward: 8.300045512703974\n",
            "Episode 961, Total Reward: -12.9056851028012\n",
            "Episode 971, Total Reward: 29.095786818536\n",
            "Episode 981, Total Reward: 192.05146123677963\n",
            "Episode 991, Total Reward: -78.11329326326774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved as lunar_lander.mp4\n"
          ]
        }
      ]
    }
  ]
}