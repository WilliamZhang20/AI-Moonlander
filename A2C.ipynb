{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOY1Jza9pkxtXb/9Z1GL+iU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xlgu2WZiBShC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "18d67836-0afa-4f4b-c75e-131dd0406167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,090 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install -y build-essential swig libopenmpi-dev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "Zyr5Td55Bbw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1869050a-f61c-4c4c-9634-256b3f205396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351172 sha256=854102401aa689184c423782db50195956b336934d9ea3130a750c9a72f2d6ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import imageio\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        action_probs = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        return action_probs, value\n",
        "\n",
        "def record_video(agent, filename=\"lunar_lander.mp4\", max_steps=1000):\n",
        "    env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "    state, _ = env.reset()\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        state_tensor = torch.FloatTensor(state)\n",
        "        action_probs, value = agent(state_tensor)\n",
        "        action_dist = torch.distributions.Categorical(action_probs)\n",
        "        action = action_dist.sample()\n",
        "\n",
        "        state, _, terminated, truncated, _ = env.step(action.item())\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(filename, frames, fps=30)\n",
        "    print(f\"Video saved as {filename}\")\n",
        "\n",
        "def a2c(env, num_episodes, lr=0.001, gamma=0.99):\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = ActorCritic(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    score_track = deque(maxlen=100)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_rewards = []\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "\n",
        "        score = 0\n",
        "\n",
        "        while not done:\n",
        "            state_tensor = torch.FloatTensor(state)\n",
        "            action_probs, value = model(state_tensor)\n",
        "            action_dist = torch.distributions.Categorical(action_probs)\n",
        "            action = action_dist.sample()\n",
        "            log_prob = action_dist.log_prob(action)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_rewards.append(reward)\n",
        "            score += reward\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "            state = next_state\n",
        "\n",
        "        R = 0\n",
        "        returns = []\n",
        "        for r in reversed(rewards):\n",
        "            R = r + gamma * R\n",
        "            returns.insert(0, R)\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + 1e-8) # Normalize returns\n",
        "\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "\n",
        "        for log_prob, value, ret in zip(log_probs, values, returns):\n",
        "            advantage = ret - value.squeeze()\n",
        "            actor_loss = -log_prob * advantage\n",
        "            critic_loss = (ret - value.squeeze()) ** 2\n",
        "\n",
        "            actor_losses.append(actor_loss)\n",
        "            critic_losses.append(critic_loss)\n",
        "\n",
        "        actor_loss = torch.stack(actor_losses).mean()\n",
        "        critic_loss = torch.stack(critic_losses).mean()\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_reward = sum(episode_rewards)\n",
        "        score_track.append(score)\n",
        "        avg_reward = np.mean(score_track)\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode {episode + 1}, Total Reward: {total_reward:.2f}, Avg Reward: {avg_reward:.2f}\")\n",
        "\n",
        "        if avg_reward >= 200:\n",
        "            print('Solved!')\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"LunarLander-v3\")\n",
        "    model = a2c(env, num_episodes=1000)\n",
        "    env.close()\n",
        "    record_video(model)"
      ],
      "metadata": {
        "id": "p93yxK50BhWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7e81df7-f3c1-4634-c4c7-7ffeb0c2bcdb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -386.73, Avg Reward: -386.73\n",
            "Episode 11, Total Reward: -187.78, Avg Reward: -233.78\n",
            "Episode 21, Total Reward: -173.35, Avg Reward: -199.66\n",
            "Episode 31, Total Reward: -282.48, Avg Reward: -192.10\n",
            "Episode 41, Total Reward: -223.79, Avg Reward: -191.95\n",
            "Episode 51, Total Reward: -269.91, Avg Reward: -188.12\n",
            "Episode 61, Total Reward: -85.55, Avg Reward: -190.43\n",
            "Episode 71, Total Reward: -147.63, Avg Reward: -180.41\n",
            "Episode 81, Total Reward: -89.54, Avg Reward: -169.56\n",
            "Episode 91, Total Reward: -25.66, Avg Reward: -159.30\n",
            "Episode 101, Total Reward: -106.18, Avg Reward: -149.24\n",
            "Episode 111, Total Reward: -3.54, Avg Reward: -135.61\n",
            "Episode 121, Total Reward: -36.94, Avg Reward: -128.80\n",
            "Episode 131, Total Reward: -165.78, Avg Reward: -120.40\n",
            "Episode 141, Total Reward: -70.91, Avg Reward: -111.72\n",
            "Episode 151, Total Reward: -80.83, Avg Reward: -112.10\n",
            "Episode 161, Total Reward: 6.75, Avg Reward: -95.16\n",
            "Episode 171, Total Reward: -27.80, Avg Reward: -87.41\n",
            "Episode 181, Total Reward: -17.55, Avg Reward: -79.17\n",
            "Episode 191, Total Reward: -34.32, Avg Reward: -75.54\n",
            "Episode 201, Total Reward: -253.60, Avg Reward: -71.63\n",
            "Episode 211, Total Reward: -66.57, Avg Reward: -65.39\n",
            "Episode 221, Total Reward: -47.37, Avg Reward: -59.14\n",
            "Episode 231, Total Reward: -69.19, Avg Reward: -58.83\n",
            "Episode 241, Total Reward: 39.23, Avg Reward: -55.83\n",
            "Episode 251, Total Reward: -144.04, Avg Reward: -45.53\n",
            "Episode 261, Total Reward: -309.75, Avg Reward: -48.07\n",
            "Episode 271, Total Reward: -117.61, Avg Reward: -51.52\n",
            "Episode 281, Total Reward: -204.73, Avg Reward: -66.12\n",
            "Episode 291, Total Reward: -36.99, Avg Reward: -66.64\n",
            "Episode 301, Total Reward: -47.15, Avg Reward: -63.12\n",
            "Episode 311, Total Reward: 19.58, Avg Reward: -62.10\n",
            "Episode 321, Total Reward: 19.90, Avg Reward: -56.47\n",
            "Episode 331, Total Reward: -49.88, Avg Reward: -49.58\n",
            "Episode 341, Total Reward: -12.52, Avg Reward: -47.87\n",
            "Episode 351, Total Reward: 52.09, Avg Reward: -40.26\n",
            "Episode 361, Total Reward: 18.73, Avg Reward: -36.09\n",
            "Episode 371, Total Reward: -26.50, Avg Reward: -26.92\n",
            "Episode 381, Total Reward: 30.67, Avg Reward: -11.35\n",
            "Episode 391, Total Reward: 3.74, Avg Reward: -5.75\n",
            "Episode 401, Total Reward: 3.10, Avg Reward: -0.78\n",
            "Episode 411, Total Reward: 103.48, Avg Reward: 2.56\n",
            "Episode 421, Total Reward: 59.09, Avg Reward: 2.82\n",
            "Episode 431, Total Reward: 132.33, Avg Reward: 10.60\n",
            "Episode 441, Total Reward: 82.89, Avg Reward: 24.57\n",
            "Episode 451, Total Reward: 107.49, Avg Reward: 28.00\n",
            "Episode 461, Total Reward: -93.03, Avg Reward: 31.89\n",
            "Episode 471, Total Reward: 88.92, Avg Reward: 35.44\n",
            "Episode 481, Total Reward: 106.92, Avg Reward: 44.02\n",
            "Episode 491, Total Reward: 45.68, Avg Reward: 52.25\n",
            "Episode 501, Total Reward: 74.60, Avg Reward: 57.02\n",
            "Episode 511, Total Reward: 158.88, Avg Reward: 65.98\n",
            "Episode 521, Total Reward: 30.83, Avg Reward: 65.93\n",
            "Episode 531, Total Reward: 39.12, Avg Reward: 65.77\n",
            "Episode 541, Total Reward: 206.62, Avg Reward: 62.49\n",
            "Episode 551, Total Reward: 221.17, Avg Reward: 67.49\n",
            "Episode 561, Total Reward: 262.45, Avg Reward: 83.95\n",
            "Episode 571, Total Reward: 284.97, Avg Reward: 101.70\n",
            "Episode 581, Total Reward: 209.75, Avg Reward: 112.02\n",
            "Episode 591, Total Reward: 255.13, Avg Reward: 118.16\n",
            "Episode 601, Total Reward: 27.18, Avg Reward: 121.97\n",
            "Episode 611, Total Reward: 227.91, Avg Reward: 120.18\n",
            "Episode 621, Total Reward: 196.29, Avg Reward: 131.13\n",
            "Episode 631, Total Reward: 233.13, Avg Reward: 145.38\n",
            "Episode 641, Total Reward: 165.93, Avg Reward: 160.02\n",
            "Episode 651, Total Reward: -190.12, Avg Reward: 163.06\n",
            "Episode 661, Total Reward: -99.05, Avg Reward: 156.00\n",
            "Episode 671, Total Reward: 211.18, Avg Reward: 141.64\n",
            "Episode 681, Total Reward: 59.11, Avg Reward: 140.19\n",
            "Episode 691, Total Reward: 22.11, Avg Reward: 137.44\n",
            "Episode 701, Total Reward: 225.80, Avg Reward: 135.99\n",
            "Episode 711, Total Reward: -65.33, Avg Reward: 138.62\n",
            "Episode 721, Total Reward: 184.67, Avg Reward: 132.52\n",
            "Episode 731, Total Reward: 160.04, Avg Reward: 114.78\n",
            "Episode 741, Total Reward: -0.23, Avg Reward: 94.59\n",
            "Episode 751, Total Reward: 215.34, Avg Reward: 96.89\n",
            "Episode 761, Total Reward: 276.74, Avg Reward: 102.19\n",
            "Episode 771, Total Reward: 257.06, Avg Reward: 111.89\n",
            "Episode 781, Total Reward: 0.53, Avg Reward: 111.02\n",
            "Episode 791, Total Reward: -149.00, Avg Reward: 105.03\n",
            "Episode 801, Total Reward: -108.88, Avg Reward: 86.83\n",
            "Episode 811, Total Reward: -106.73, Avg Reward: 68.81\n",
            "Episode 821, Total Reward: 145.65, Avg Reward: 69.33\n",
            "Episode 831, Total Reward: -67.48, Avg Reward: 70.82\n",
            "Episode 841, Total Reward: 234.55, Avg Reward: 73.57\n",
            "Episode 851, Total Reward: -156.63, Avg Reward: 69.39\n",
            "Episode 861, Total Reward: 294.89, Avg Reward: 65.73\n",
            "Episode 871, Total Reward: 205.03, Avg Reward: 69.98\n",
            "Episode 881, Total Reward: 237.26, Avg Reward: 71.14\n",
            "Episode 891, Total Reward: 258.70, Avg Reward: 84.08\n",
            "Episode 901, Total Reward: 11.60, Avg Reward: 100.48\n",
            "Episode 911, Total Reward: 253.72, Avg Reward: 120.98\n",
            "Episode 921, Total Reward: 255.87, Avg Reward: 138.13\n",
            "Episode 931, Total Reward: 233.46, Avg Reward: 157.37\n",
            "Episode 941, Total Reward: 249.52, Avg Reward: 176.99\n",
            "Episode 951, Total Reward: 225.43, Avg Reward: 191.26\n",
            "Solved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved as lunar_lander.mp4\n"
          ]
        }
      ]
    }
  ]
}